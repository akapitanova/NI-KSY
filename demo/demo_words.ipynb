{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:32:12.224877100Z",
     "start_time": "2024-01-16T13:32:07.887904500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:32:12.250374600Z",
     "start_time": "2024-01-16T13:32:12.229407200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "\n",
    "image_width = 475\n",
    "image_height = 100\n",
    "# Convolutional layer parameters\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "channels = 32\n",
    "\n",
    "# Calculate output size after convolutional layer\n",
    "conv_output_width = math.floor((image_width - kernel_size + 2 * padding) / stride) + 1\n",
    "conv_output_height = math.floor((image_height - kernel_size + 2 * padding) / stride) + 1\n",
    "\n",
    "# Calculate output size after max pooling layer\n",
    "pool_output_width = math.floor(conv_output_width / 2)\n",
    "pool_output_height = math.floor(conv_output_height / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:32:12.282055800Z",
     "start_time": "2024-01-16T13:32:12.246234200Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(pool_output_width * pool_output_height * channels, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, pool_output_width * pool_output_height * channels)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T13:32:55.835759500Z",
     "start_time": "2024-01-16T13:32:55.031441900Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    return Image.open(path)\n",
    "# load model\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load('binary_classifier_cnn_words.pth'))\n",
    "\n",
    "# load our and foreign image:\n",
    "to_classif = read_image('./our1.png')\n",
    "\n",
    "# convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# transform image\n",
    "to_classif = transform(to_classif)\n",
    "\n",
    "# evaluate images by model\n",
    "to_classif = to_classif.unsqueeze(0)\n",
    "output = model(to_classif)\n",
    "\n",
    "# evaluate output\n",
    "output = torch.squeeze(output)\n",
    "output = (output > 0).int()\n",
    "\n",
    "\n",
    "# print results\n",
    "if output == 1:\n",
    "    print(\"It is our word!\")\n",
    "else:\n",
    "    print(\"It is foreign word!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
