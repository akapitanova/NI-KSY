{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXJxkc2_AguP"
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0hobcehCRlp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAJ0zUpdAm98"
   },
   "source": [
    "# Data preprossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saI77vxKAy2A"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQUDF5efAyrx"
   },
   "outputs": [],
   "source": [
    "train_dir = './dataset_letters/train'\n",
    "test_dir = './dataset_letters/test'\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAT7z4QpC4WU"
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = ImageFolder(test_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wauxgc0_AwGI"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inab5UGx0uVU"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "\n",
    "image_width = 28\n",
    "image_height = 28\n",
    "# Convolutional layer parameters\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "channels = 32\n",
    "\n",
    "# Calculate output size after convolutional layer\n",
    "conv_output_width = math.floor((image_width - kernel_size + 2 * padding) / stride) + 1\n",
    "conv_output_height = math.floor((image_height - kernel_size + 2 * padding) / stride) + 1\n",
    "\n",
    "# Calculate output size after max pooling layer\n",
    "pool_output_width = math.floor(conv_output_width / 2)\n",
    "pool_output_height = math.floor(conv_output_height / 2)\n",
    "print(pool_output_width, pool_output_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xadS7gBN30zV"
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DbL8mnRVJSG"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(pool_output_width * pool_output_height * channels, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, pool_output_width * pool_output_height * channels)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18obzU3WVJN6"
   },
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjHs--fOVJJ5"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.squeeze(outputs)\n",
    "            predicted = (outputs > 0).int()  # Convert logits to binary predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f'total: {total}, correct: {correct}')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fii26uujrOae",
    "outputId": "f7434030-2b34-4f14-da77-c60469e2581a"
   },
   "outputs": [],
   "source": [
    "num_epochs = 8\n",
    "losses = []\n",
    "accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    accuracy = evaluate(model, test_loader)\n",
    "    losses.append(train_loss)\n",
    "    accuracies.append(accuracy*100)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Accuracy: {accuracy*100:.2f}%\")\n",
    "torch.save(model.state_dict(), 'binary_classifier_cnn_letters.pth')\n",
    "\n",
    "plt.gca().set_ylim(bottom=0, top=100)\n",
    "plt.plot(accuracies, label='accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
